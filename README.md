# Employee Hunter AI

A Flask-based web application for CV intake, AI extraction, and intelligent candidate matching using similarity search. Upload CVs, search by job description, and get matched candidates ranked by relevance.

## Features

- **CV Upload & Parsing**: Support for PDF, DOCX, DOC, and TXT files with multi-layer extraction (pdfplumber ‚Üí PyPDF2 ‚Üí raw fallback)
- **AI-Powered Extraction**: Uses OpenAI GPT-3.5-turbo or Claude Haiku 4.5 to extract:
  - Personal info (name, email, phone, address, location)
  - Professional info (role, experience, company, summary)
  - Education (degree, university, qualifications)
  - Technical skills (languages, frameworks, databases, cloud platforms)
- **Intelligent Search**: TF-IDF + cosine similarity matching to find best candidate fits
- **Scalable Database**: ChromaDB persistent storage with server-side metadata fetching
- **Provider Switching**: Easy toggle between OpenAI and Claude via `LLM_PROVIDER` env var
- **Robust Fallbacks**: Enhanced local extractors kick in if LLM calls fail or return malformed JSON

## Project Structure

```
employee_hunter_ai/
‚îú‚îÄ‚îÄ app.py                              # Flask entrypoint: routes, session handling
‚îú‚îÄ‚îÄ config.py                           # Environment variables, helpers
‚îú‚îÄ‚îÄ requirements.txt                    # Pinned dependencies
‚îú‚îÄ‚îÄ README.md                           # This file
‚îú‚îÄ‚îÄ cv_database/                        # ChromaDB persistent storage (auto-created)
‚îÇ   ‚îî‚îÄ‚îÄ chroma.sqlite3
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ uploads/                        # Uploaded CV files (temporary)
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ index.html                      # Home page
‚îÇ   ‚îú‚îÄ‚îÄ upload.html                     # CV upload form
‚îÇ   ‚îú‚îÄ‚îÄ results.html                    # Search results grid
‚îÇ   ‚îî‚îÄ‚îÄ candidate_profile.html          # Detailed candidate profile
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ai_matcher.py               # Pipeline orchestration
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chroma_db.py                # ChromaDB wrapper + TF-IDF search
‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_client.py            # OpenAI ChatCompletion integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claude_client.py            # Claude (Anthropic) HTTP client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cv_analyzer.py              # (legacy, optional)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ skills_extractor.py         # (legacy, optional)
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ file_parser.py              # PDF/DOCX/TXT extraction
‚îÇ       ‚îî‚îÄ‚îÄ text_cleaner.py             # Text normalization
```

## Quick Start

### 1. Prerequisites
- Python 3.10+ (tested with 3.14.0)
- Git (optional, for version control)

### 2. Installation

Clone or download the repo and navigate to the project root:

```bash
cd employee_hunter_ai
```

Create and activate a virtual environment:

```powershell
# Windows PowerShell
python -m venv .venv
.venv\Scripts\Activate.ps1

# Linux/Mac bash
python3 -m venv .venv
source .venv/bin/activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

### 3. Configuration

Create a `.env` file in the project root with your API keys:

```plaintext
OPENAI_API_KEY=sk-your-openai-key-here
ANTHROPIC_API_KEY=your-anthropic-key-here
LLM_PROVIDER=openai
```

**Options for `LLM_PROVIDER`:**
- `openai` (default) ‚Äî Uses OpenAI GPT-3.5-turbo for extraction
- `claude` ‚Äî Uses Claude Haiku 4.5 from Anthropic for extraction

If `ANTHROPIC_API_KEY` is not set and `LLM_PROVIDER=claude`, the app will fall back to OpenAI.

### 4. Run the App

```bash
python app.py
```

The app will start on **http://0.0.0.0:5000** (or http://localhost:5000 in your browser).

## Usage

### Upload CVs

1. Go to **"Upload CV"** in the navigation
2. Select a candidate name and upload a PDF/DOCX/DOC/TXT file
3. The app will:
   - Parse the file to extract text
   - Clean and normalize the text
   - Send to LLM for comprehensive analysis
   - Store metadata and raw CV in the database
4. You'll see a confirmation with detected skills and metadata

### Search & Match

1. Go to **"Search"** (home page)
2. Paste a job description (e.g., "Looking for a Python developer with AWS experience")
3. Click **"Find Matching Candidates"**
4. Results show all matching CVs ranked by relevance (65‚Äì95% match score)
5. Click any candidate card to view the full profile with:
   - Contact info (email, phone, address)
   - Professional experience and current role
   - Education and certifications
   - Technical skills by category
   - CV preview (first 800 characters)

### Debug & Inspection

Use these debug endpoints to inspect the database:

- **`GET /debug/database`** ‚Äî Full database contents (CV IDs, names, skills)
- **`GET /debug/cv_count`** ‚Äî Total number of stored CVs
- **`GET /debug/clear_database`** ‚Äî Clear all stored CVs (‚ö†Ô∏è destructive)

Example:

```bash
curl http://localhost:5000/debug/cv_count
# Output: {"total_cvs": 5}
```

## Data Flow

```
Upload CV
    ‚Üì
Parse file (pdfplumber ‚Üí PyPDF2 ‚Üí raw)
    ‚Üì
Clean text (normalize, remove special chars)
    ‚Üì
LLM Analysis (OpenAI or Claude)
    ‚îú‚Üí extract_skills()
    ‚îî‚Üí extract_comprehensive_details() ‚Üí JSON schema
    ‚Üì
Format metadata (education, summary, preview)
    ‚Üì
Store in ChromaDB + session
    ‚Üì
[Database Ready]

Search Job Description
    ‚Üì
TF-IDF vectorization + cosine similarity
    ‚Üì
Filter by threshold (0.15)
    ‚Üì
Remap scores to 65‚Äì95% range
    ‚Üì
Return ranked results
    ‚Üì
Store IDs + distances in session
    ‚Üì
[Results Ready]

Click Candidate
    ‚Üì
Fetch metadata from DB by ID
    ‚Üì
Render profile page
    ‚Üì
[Profile Loaded - Always correct data]
```

## Metadata Keys

When a CV is stored, the following metadata is extracted and available:

| Key | Description |
|-----|-------------|
| `candidate_name` | Full name of candidate |
| `email` | Email address |
| `phone` | Phone number with country code |
| `address` | Full postal address |
| `location` | City, country |
| `current_role` | Current job title |
| `experience` | Years of experience (e.g., "5 years") |
| `current_company` | Current employer name |
| `education` | Highest degree + university (bullet points) |
| `summary` | Professional summary (bullet points) |
| `programming_languages` | Comma-separated list |
| `frameworks` | Comma-separated list |
| `databases` | Comma-separated list |
| `cloud_platforms` | Comma-separated list |
| `skills` | Comma-separated list of all skills |
| `raw_text` | First ~800 chars of CV (formatted preview) |

## Search Configuration

To adjust search recall/precision, edit `src/database/chroma_db.py`:

```python
# Line ~80 in chroma_db.py
threshold = 0.15  # Increase for stricter matching, decrease for looser
max_features = 1000  # Reduce for fewer features (faster but less accurate)
```

Score remapping (65‚Äì95%) happens on lines ~95-98:

```python
score = 65 + (sim * 30)  # Adjust constants to change range
```

## LLM Integration

### OpenAI (Default)

- **Model**: `gpt-3.5-turbo`
- **Max tokens**: 800 (skills), 1500 (comprehensive)
- **Temperature**: 0.3 (skills), 0.1 (comprehensive)
- **Fallback**: Enhanced local regex + pattern matching if LLM fails

### Claude (Anthropic)

- **Model**: `claude-haiku-4.5`
- **API**: HTTP POST to `https://api.anthropic.com/v1/complete`
- **Max tokens**: Same as OpenAI
- **Temperature**: Same as OpenAI
- **Fallback**: Automatically calls `OpenAIClient` enhanced extractors

**To use Claude:**

1. Set `ANTHROPIC_API_KEY` in `.env`
2. Set `LLM_PROVIDER=claude` in `.env`
3. Restart the app

The app will switch seamlessly at runtime.

## Troubleshooting

### "No candidate data available. Please perform a search first."

**Cause**: Session cookie was too large and truncated (when >3 CVs in results).

**Fix** (already applied): Session now stores only lightweight IDs + distances. Metadata is fetched server-side by ID on profile load.

**What to do**:
1. Clear browser cookies for localhost:5000
2. Restart the app
3. Re-run your search

### "ModuleNotFoundError: No module named 'chromadb'"

**Cause**: Dependencies not installed in the active venv.

**Fix**:
```bash
.venv\Scripts\Activate.ps1    # Ensure venv is active
pip install -r requirements.txt
```

### "OPENAI_API_KEY not set"

**Cause**: Missing or incorrect API key in `.env`.

**Fix**:
1. Create/edit `.env` in project root
2. Add: `OPENAI_API_KEY=sk-your-key`
3. Save and restart the app

### Search returns no results

**Cause**: Job description too different from CV content, or similarity threshold too high.

**Fix**:
1. Try a simpler job description (e.g., "Python developer" instead of very specific requirements)
2. Lower the threshold in `src/database/chroma_db.py` (line ~80: `threshold = 0.10`)
3. Ensure CVs are uploaded and database contains data: `GET /debug/cv_count`

### Profile shows "Email in CV" for all candidates

**Cause**: LLM extraction failed; fallback regex didn't find valid email.

**Fix**: 
1. Check server logs for OpenAI errors
2. Verify CV file is readable (try uploading a test PDF)
3. Check that OPENAI_API_KEY is valid

## Dependencies

See `requirements.txt` for full list. Key packages:

- **flask** 2.3.3 ‚Äî Web framework
- **chromadb** 0.4.15 ‚Äî Vector database
- **openai** 1.3.0 ‚Äî OpenAI API client
- **pdfplumber** 0.9.0 ‚Äî PDF extraction
- **PyPDF2** 3.0.1 ‚Äî PDF fallback
- **scikit-learn** 1.5.2 ‚Äî TF-IDF + cosine similarity
- **requests** 2.32.3 ‚Äî HTTP client (for Claude)
- **python-dotenv** 1.0.0 ‚Äî .env parsing

## Performance Notes

- **First search** may be slow (TF-IDF vectorization on all CVs)
- **Subsequent searches** are faster (vectorizer is cached in memory)
- **Profile load** is instant (direct DB lookup by ID)
- **Large CV count** (100+): Consider batch uploads or async processing (future enhancement)

## Future Enhancements

- Async CV processing queue (Celery + Redis)
- Resume screening via LLM (ranking by custom criteria)
- Bulk upload with progress tracking
- Export search results to CSV
- User authentication + multi-user support
- API rate limiting and caching
- Mobile app

## Support & Debugging

For detailed debugging, enable logs in `app.py`:

```python
app.run(debug=True, host='0.0.0.0', port=5000)
```

Check console output for:
- ‚úÖ CV stored permanently
- üîç Search completed. Found X candidates
- üë§ Loading profile for index: N
- ‚ùå Error messages with traceback

## License

This project is provided as-is. Modify and distribute freely.

## Authors

- AI-enhanced by Claude Haiku 4.5 (Anthropic)
- Built with Flask, ChromaDB, OpenAI/Claude, scikit-learn

---

**Version**: 1.0.0  
**Last Updated**: November 28, 2025
